<p align="center">
    <h1 align="center">ğŸ”¥ Data-Centric MLæ–¹å‘</h1>
    <p align="center">å…¥é—¨æŒ‡å—</p>
    <p align="center">
        <a href="https://github.com/PKU-DAIR">
            <img alt="Static Badge" src="https://img.shields.io/badge/%C2%A9-PKU--DAIR-%230e529d?labelColor=%23003985">
        </a>
        <a href="https://github.com/PKU-DAIR">
            <img alt="Static Badge" src="https://img.shields.io/badge/PKU--DAIR-black?logo=github">
        </a>
    </p>
</p>

> æ³¨:âš¡ä¸º**åŸºç¡€å¿…è¯»**,ğŸ’ä¸º**åŸºç¡€é€‰è¯»**,ğŸ’¡ä¸º**è¿›é˜¶é˜…è¯»**


### Data Centric ML Basic

<details open>
<summary>

##### æ•°æ®ä¾§åŸºç¡€ï¼š

**Data-Centric MLæ˜¯ç ”ç©¶ä½¿ç”¨æ•°æ®ç”Ÿæˆã€é€‰æ‹©ä¸é…æ¯”ç­‰æ–¹æ³•å®ç°å¤§è§„æ¨¡ï¼Œé«˜æ•ˆä»¥åŠæå‡æ¨¡å‹è¡¨ç°çš„ç ”ç©¶æ–¹å‘ã€‚**

</summary>

- `âš¡` ğŸ—ï¸ [MIT DCAI 2024](https://dcai.csail.mit.edu/)
> è¯¾ç¨‹éš¾åº¦ä¸å¤§ï¼Œä½†æ˜¯æ¶µç›–äº†å¾ˆå¤š DCAI çš„ Topicï¼ŒåŒ…æ‹¬ä½†ä¸é™äºï¼šData Selection/Data Cleaning/Distribution Shift/Data Curation ç­‰ï¼Œæ¯èŠ‚è¯¾éƒ½æœ‰ä¸€ä¸ªé…å¥—çš„å®éªŒï¼Œå®Œæˆéš¾åº¦ä¹Ÿä¸å¤§ï¼Œå®Œæˆåå¯ä»¥å¯¹è¯¥topicæœ‰ä¸€ä¸ªæ¯”è¾ƒ general çš„è®¤è¯†
- `âš¡` ğŸ—ï¸ [åç››é¡¿å¤§å­¦ DCAI](https://koh.pw/cse599j/)

</details>

### LLM and VLM Basic

<details open>
<summary>

##### Quick Start

</summary>

- `âš¡` ğŸ“„+ğŸ”§ [å¤šæ¨¡æ€å¤§æ¨¡å‹å…¥é—¨æ–‡æ¡£](https://wcny4qa9krto.feishu.cn/wiki/Lyq9wNeovivXpbkwaNVcebrCnnb)
- `âš¡` ğŸ“„ [Survey](https://arxiv.org/pdf/2306.13549)

</details>

<details open>
<summary>

##### ImageLLMs

</summary>

- `âš¡` ğŸ’­ [BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://arxiv.org/abs/2301.12597)
- `âš¡` ğŸ’­ [MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models](https://arxiv.org/abs/2304.10592)
- `âš¡` ğŸ’­ [Visual Instruction Tuning](https://arxiv.org/abs/2304.08485)
- `âš¡` ğŸ’­ [Improved Baselines with Visual Instruction Tuning](https://arxiv.org/abs/2310.03744)

</details>

<details open>
<summary>

##### VideoLLMs

</summary>

- `âš¡` ğŸ’­ [Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond](https://arxiv.org/abs/2308.12966)
- `âš¡` ğŸ’­ [Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding](https://arxiv.org/abs/2306.02858)
- `âš¡` ğŸ’­ [Video-LLaVA: Learning United Visual Representation by Alignment Before Projection](https://arxiv.org/abs/2311.10122)
- `âš¡` ğŸ’­ [MVBench: A Comprehensive Multi-modal Video Understanding Benchmark](https://arxiv.org/abs/2311.17005)
- `âš¡` ğŸ’­ [InternVideo2: Scaling Foundation Models for Multimodal Video Understanding](https://arxiv.org/abs/2403.15377)

> å¯èƒ½æœ‰å¸®åŠ©çš„[æ–‡æ¡£](https://docs.google.com/document/d/13iqTmfJZVt8Mk3yt9icXQq_Nvbkf99PGIKJaKJBb_2c/edit?usp=sharing)

</details>

<details open>
<summary>

##### Data-Centric VLMsï¼ˆä¸»è¦æ˜¯æ•°æ®çš„é€‰æ‹©ã€å¢å¼º(recaption)ï¼Œæ¯”å¦‚é‡å†™captionï¼‰
</summary>

- `âš¡` ğŸ’­ [Improved Baselines with Visual Instruction Tuning](https://arxiv.org/abs/2310.03744)
- `âš¡` ğŸ’­ [Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models](https://arxiv.org/abs/2311.06607)
- `âš¡` ğŸ’­ [ShareGPT4V: Improving Large Multi-Modal Models with Better Captions](https://arxiv.org/abs/2311.12793)
- `âš¡` ğŸ’­ [Video-LLaVA: Learning United Visual Representation by Alignment Before Projection](https://arxiv.org/abs/2311.10122)
- `âš¡` ğŸ’­ [MVBench: A Comprehensive Multi-modal Video Understanding Benchmark](https://arxiv.org/abs/2311.17005)
- `âš¡` ğŸ’­ [MiniGPT4-Video: Advancing Multimodal LLMs for Video Understanding with Interleaved Visual-Textual Tokens](https://arxiv.org/abs/2404.03413)
> è§è¿™äº›è®ºæ–‡ä½¿ç”¨çš„æ•°æ®ï¼ŒåŒæ—¶è¿™ä¸‰ä¸ªè®ºæ–‡çš„æ¨¡å‹å’Œæ•°æ®é‡å¯ä»¥è®­ç»ƒï¼Œå…¶ä½™æ¨¡å‹è®­ç»ƒä¸å‡ºæ¥ï¼ˆDatasetsï¼‰
</details>

<details open>
<summary>

##### Vision-Language Model Visual Information Extraction
</summary>

**1ï¼‰ä¸»æµçš„VLMæ™®éä½¿ç”¨CLIP-ViTä½œä¸ºVision Encoderï¼Œé€šè¿‡å¯¹æ¯”å­¦ä¹ è®­ç»ƒçš„è§†è§‰ç¼–ç å™¨æ— æ³•æå–ç»†ç²’åº¦çš„å›¾åƒç‰¹å¾ï¼Œå•çº¯å¢åŠ å›¾åƒåˆ†è¾¨ç‡å¯¹è¾“å…¥çª—å£å½±å“è¿‡å¤§ï¼Œå› æ­¤ä¸åŒVEçš„èåˆï¼ˆæ¯”å¦‚DINOï¼ŒSigLipç­‰ï¼‰ä¸é€‰æ‹©æ˜¯å¿…è¦çš„ã€‚**

**2ï¼‰å½“å‰çš„é«˜åˆ†è¾¨ç‡VLLMå°†å›¾åƒç¼–ç ä¸ºé«˜åˆ†è¾¨ç‡çš„visual tokenï¼Œè®¡ç®—æˆæœ¬è¿‡å¤§ã€‚å¦‚ä½•é€‰æ‹©ç‰¹å®šçš„ã€é—®é¢˜ç›¸å…³çš„é«˜åˆ†è¾¨ç‡è§†è§‰ç‰¹å¾ï¼Œè®¾è®¡çµæ´»é«˜æ•ˆçš„æ³¨æ„åŠ›æœºåˆ¶æ˜¯å¿…è¦çš„ã€‚**

- `âš¡` ğŸ“„+ğŸ”§ [BRAVE: Broadening the visual encoding of vision-language models](https://brave-vlms.epfl.ch/)
- `âš¡` ğŸ“„+ğŸ”§ [Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs](https://arxiv.org/abs/2401.06209)
- `âš¡` ğŸ“„+ğŸ”§ [FlexAttention for Efficient High-Resolution Vision-Language Models](https://arxiv.org/abs/2407.20228)
- `âš¡` ğŸ“„+ğŸ”§ [From CLIP to DINO: Visual Encoders Shout in Multi-modal Large Language Models](https://arxiv.org/abs/2310.08825)

> 1. ä¸åŒvision encoderçš„è§†è§‰ç‰¹å¾åˆ†æä¸ç»„åˆ

> 2. Question-awareçš„é«˜åˆ†è¾¨ç‡è§†è§‰ç‰¹å¾çš„æå–ä¸æ³¨å…¥

</details>

<details open>
<summary>

##### Modality Alignment
</summary>

**1ï¼‰åœ¨å½“å‰çš„X-to-Tå¤šæ¨¡æ€ç†è§£æ¨¡å‹ä¸­ï¼Œé€šå¸¸ä½¿ç”¨ç®€å•çš„æ¨¡å—(å¦‚MLP)ä½œä¸ºè·¨æ¨¡æ€è¡¨ç¤ºçš„æ¡¥æ¢ï¼Œä½†è¿™äº›æ–¹æ³•å¾€å¾€å› ä¸ºç®€å•çš„ç©ºé—´æ˜ å°„å¯¼è‡´ä¸åŒæ¨¡æ€çš„è¡¨å¾ä¹‹é—´äº§ç”Ÿåå·®ï¼Œä»è€Œå½±å“æ¨¡å‹çš„æ¨ç†å’Œç”Ÿæˆã€‚**

**2ï¼‰è®¾è®¡æ›´å¤æ‚ä¸”é«˜æ•ˆçš„è·¨æ¨¡æ€å¯¹é½æœºåˆ¶ï¼Œå°¤å…¶æ˜¯åœ¨ä¸åŒæ¨¡æ€é—´çš„å¯¹é½æœºåˆ¶ä¸Šè¿›è¡Œä¼˜åŒ–æ˜¯å¿…è¦çš„ã€‚**

- `âš¡` ğŸ“„+ğŸ”§ https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models?tab=readme-ov-file
- `âš¡` ğŸ“„+ğŸ”§ https://arxiv.org/pdf/2408.05211

> 1. åˆ›æ–°æ€§çš„è®­ç»ƒèŒƒå¼

> 2. Modality Projector / Resamplerçš„è®¾è®¡ï¼ˆæ‹¼æ¥åè¾“å…¥æˆ–llmå†…éƒ¨æ³¨å…¥ï¼‰

> 3. è·¨æ¨¡æ€é—´äº¤äº’çš„å¯è§£é‡Šæ€§

</details>

### Data Centric LLM and VLM Algorithms

- `âš¡` ğŸ’­ [A Survey of Multimodal Large Language Model from A Data-centric Perspective](https://arxiv.org/abs/2405.16640)

<details open>
<summary>

##### Data Processing
</summary>

</details>

<details open>
<summary>


##### Data Evaluation
</summary>

- `âš¡` ğŸ’­ [Awesome Data Evaluation](https://github.com/Open-DataFlow/Open-DataFlow-Eval/blob/main/Awesome_Data_Evaluation.md)
  
</details>

<details open>
<summary>

##### Data Generation
</summary>

- `âš¡` ğŸ’­ [Comprehensive Exploration of Synthetic Data Generation: A Survey](https://arxiv.org/abs/2401.02524)
-  `âš¡` ğŸ’­ [On LLMs-Driven Synthetic Data Generation, Curation, and Evaluation: A Survey](https://arxiv.org/abs/2406.15126)
  
</details>

<details open>
<summary>

##### Data Extraction(RAG)
</summary>

- `âš¡` ğŸ’­ [Retrieval-Augmented Generation for AI-Generated Content: A Survey](https://arxiv.org/abs/2402.19473)
- `âš¡` ğŸ’­ [GraphRAG](https://github.com/microsoft/graphrag)
- `âš¡` ğŸ’­ [Raptor](https://arxiv.org/abs/2401.18059v1)
- `âš¡` ğŸ’­ [QAEncoder: Towards Aligned Representation Learning in Question Answering System](https://arxiv.org/abs/2409.20434)

</details>

### Data Centric LLM and VLM Systems
> 1ï¼‰Data Centric AIæ•°æ®ä¾§å¼€å‘ï¼Œä¸»è¦å…³æ³¨Data Juicerï¼Œè¦æ±‚å¯¹äºæ•°æ®ä¾§æœ‰æ¯”è¾ƒè¯¦ç»†çš„äº†è§£
> 2ï¼‰Data Centric AIæ•°æ®ä¾§å·¥å…·ç›¸å…³çš„ç§‘ç ”
éœ€è¦å¯¹äºæ•°æ®ä¾§å·¥å…·æœ‰ä¸€å®šçš„äº†è§£ï¼Œé˜…è¯»ç›¸å…³çš„è®ºæ–‡
é™„å½•ï¼šæ•°æ®è¯„ä¼°ç›¸å…³è®ºæ–‡

<details open>
<summary>

##### Imageæ•°æ®è¯„ä¼°è®ºæ–‡

</summary>

- `âš¡` ğŸ“„+ğŸ”§ [CLIPScore: A Reference-free Evaluation Metric for Image Captioning](https://arxiv.org/abs/2104.08718)
- `âš¡` ğŸ“„+ğŸ”§ [InfoMetIC: An Informative Metric for Reference-free Image Caption Evaluation](https://aclanthology.org/2023.acl-long.178.pdf)
- `âš¡` ğŸ“„+ğŸ”§ [leverage large multimodal model](https://arxiv.org/pdf/2406.06004)
- `âš¡` ğŸ“„+ğŸ”§ [Videoæ•°æ®è¯„ä¼°è®ºæ–‡](https://arxiv.org/pdf/2407.18589)

- `âš¡` ğŸ“„+ğŸ”§ [EMScore: Evaluating Video Captioning via Coarse-Grained and Fine-Grained Embedding Matching](https://arxiv.org/abs/2111.08919)

- `âš¡` ğŸ“„+ğŸ”§ [Positive-Augmented Contrastive Learning for Image and Video Captioning Evaluation](https://arxiv.org/abs/2303.12112)

</details>


### Data-Centric Domain-Specific LLMs
<details open>
<summary>

##### MathLLMs
</summary>

- `âš¡` ğŸ“„+ğŸ”§ [DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models](https://arxiv.org/abs/2402.03300)
- `âš¡` ğŸ“„+ğŸ”§ [InternLM-Math: Open Math Large Language Models Toward Verifiable Reasoning](https://arxiv.org/abs/2402.06332)
- `âš¡` ğŸ“„+ğŸ”§ [MathBench: Evaluating the Theory and Application Proficiency of LLMs with a Hierarchical Mathematics Benchmark](https://arxiv.org/abs/2405.12209)

</details>

<details open>
<summary>

##### Math Datasetsï¼š

</summary>

- `âš¡` ğŸ“„+ğŸ”§ [DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models](https://arxiv.org/abs/2402.03300)
- `âš¡` ğŸ“„+ğŸ”§ [InternLM-Math: Open Math Large Language Models Toward Verifiable Reasoning](https://arxiv.org/abs/2402.06332)

> æ•°å­¦çŸ¥è¯†æŒ‰ç…§éš¾åº¦åˆ†ä¸ºå°å­¦ï¼Œåˆä¸­ï¼Œé«˜ä¸­å’Œå¤§å­¦ã€‚å¤§å­¦åˆæœ‰å¾ˆå¤šç»†åˆ†ï¼Œæ¯”å¦‚æ•°å­¦åˆ†æï¼Œé«˜ç­‰ä»£æ•°ä»¥åŠæ¦‚ç‡è®ºä¸æ•°ç†ç»Ÿè®¡ï¼Œæœ€ä¼˜åŒ–æ–¹æ³•ï¼Œæ•°å€¼åˆ†æç­‰ç­‰ç›®å‰è§‚å¯Ÿåˆ°çš„ç°è±¡æ˜¯è¯´ï¼Œæ— è®ºæ˜¯å¦æ˜¯Mathçš„æ¨¡å‹ï¼Œåœ¨SFTä¹‹åé«˜ä¸­å’Œå¤§å­¦æ•°å­¦èƒ½åŠ›éƒ½ä¼šå¤§å¹…åº¦ä¸‹é™ï¼Œè¯´æ˜æ•°æ®é›†çš„æ„å»ºä¸åˆ©äºé«˜ä¸­å’Œå¤§å­¦æ•°å­¦èƒ½åŠ›çš„å­¦ä¹ 

</details>