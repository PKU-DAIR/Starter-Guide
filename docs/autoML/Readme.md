<p align="center">
    <h1 align="center">â±ï¸ AutoMLæ–¹å‘</h1>
    <p align="center">å…¥é—¨æŒ‡å—</p>
    <p align="center">
        <a href="https://github.com/PKU-DAIR">
            <img alt="Static Badge" src="https://img.shields.io/badge/%C2%A9-PKU--DAIR-%230e529d?labelColor=%23003985">
        </a>
        <a href="https://github.com/PKU-DAIR">
            <img alt="Static Badge" src="https://img.shields.io/badge/PKU--DAIR-black?logo=github">
        </a>
    </p>
</p>

> æ³¨:âš¡ä¸º**åŸºç¡€å¿…è¯»**,ğŸ’ä¸º**åŸºç¡€é€‰è¯»**,ğŸ’¡ä¸º**è¿›é˜¶é˜…è¯»**

### AutoML ä¸è¶…å‚æ•°ä¼˜åŒ–


<details open>
<summary>

##### AutoMLç»¼è¿°

</summary>

- `âš¡` ğŸ“„ AutoML: A Survey of the State-of-the-Art

</details>

<details open>
<summary>

##### è´å¶æ–¯ä¼˜åŒ–åŸºç¡€

</summary>

> å…ˆé€šè¿‡é˜…è¯»ä¸“æ äº†è§£è´å¶æ–¯ä¼˜åŒ–å’Œé«˜æ–¯è¿‡ç¨‹çš„åŸºæœ¬åŸç†ï¼Œç„¶åé˜…è¯»ä¸¤ç¯‡è®ºæ–‡äº†è§£SMBOçš„ç»å…¸æ–¹æ³•

- `âš¡` ğŸ“„ (BO & GP) https://zhuanlan.zhihu.com/p/139605200
- `âš¡` ğŸ“„ (SMAC) Sequential Model-Based Optimization for General Algorithm Configuration (extended version)
- `âš¡` ğŸ“„ (TPE) Algorithms for Hyper-Parameter Optimization

</details>

<details open>
<summary>

##### é»‘ç›’ä¼˜åŒ–ç³»ç»Ÿ

</summary>

> é€šè¿‡openboxè‡ªå·±æ­å»ºSMBOçš„åŸºæœ¬æµç¨‹
é˜…è¯»openboxæºä»£ç ï¼Œäº†è§£å…¶å†…éƒ¨å®ç°

- `âš¡` ğŸ› ï¸ openbox:  https://github.com/PKU-DAIR/open-box

</details>

<details open>
<summary>

##### å¤šç²¾åº¦ä¼˜åŒ–

</summary>

- `âš¡` ğŸ“„ Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization
- `âš¡` ğŸ“„ BOHB: Robust and Efficient Hyperparameter Optimization at Scale

</details>

<details open>
<summary>

##### å¤šç²¾åº¦ä¼˜åŒ–

</summary>

- `ğŸ’` ğŸ“„ DEHB: Evolutionary Hyperband for Scalable, Robust and Efficient Hyperparameter Optimization
- `ğŸ’` ğŸ“„ Efficient Automatic CASH via Rising Bandits
- `ğŸ’` ğŸ“„ (DyHPO) Supervising the Multi-Fidelity Race of Hyperparameter Configurations

</details>

<details open>
<summary>

##### AutoMLç³»ç»Ÿ

</summary>

- `âš¡` ğŸ“„ Auto-WEKA: combined selection and hyperparameter optimization of classification algorithms
- `âš¡` ğŸ“„ (auto-sklearn) Efficient and Robust Automated Machine Learning

</details>

<details open>
<summary>

##### AutoMLç³»ç»Ÿ

</summary>

> é€šè¿‡å¼€æºautoMLç³»ç»Ÿè‡ªè¡Œæ­å»ºè‡ªåŠ¨åŒ–æœºå™¨å­¦ä¹ çš„å®Œæ•´æµç¨‹ï¼Œå¹¶äº†è§£å…¶å†…éƒ¨å®ç°

- `âš¡` ğŸ› ï¸ auto-sklearn:  https://github.com/automl/auto-sklearn
- `âš¡` ğŸ› ï¸ MindWare:  https://github.com/thomas-young-2013/mindware

</details>

<details open>
<summary>

##### è¿ç§»å­¦ä¹ ç»¼è¿°

</summary>

- `âš¡` ğŸ“„ Transfer Learning for Bayesian Optimization: A Survey

</details>

<details open>
<summary>

##### è¿ç§»å­¦ä¹ 

</summary>

- `âš¡` ğŸ“„ (RGPE) Scalable Meta-Learning for Bayesian Optimization using Ranking-Weighted Gaussian Process Ensembles
- `âš¡` ğŸ“„ (TAF) Scalable Gaussian process-based transfer surrogates for hyperparameter optimization

</details>

### ç½‘ç»œç»“æ„æœç´¢(NAS)


<details open>
<summary>

##### è¿ç§»å­¦ä¹ 

</summary>

- `ğŸ’` ğŸ“„ Learning Hyperparameter Optimization Initializations
- `ğŸ’` ğŸ“„ Hyperparameter Search Space Pruning â€“ A NewComponent for Sequential Model-Based Hyperparameter Optimization
- `ğŸ’` ğŸ“„ (POGOE) Scalable Hyperparameter Optimization with Products of Gaussian Process Experts
- `ğŸ’` ğŸ“„ (TST) Two-Stage Transfer Surrogate Model for Automatic Hyperparameter Optimization
- `ğŸ’` ğŸ“„ (TransBO) TransBO: Hyperparameter Optimization via Two-Phase Transfer Learning

</details>

<details open>
<summary>

##### NASç»¼è¿°

</summary>

- `âš¡` ğŸ“„ Neural Architecture Search: A Survey

</details>

<details open>
<summary>

##### NASåŸºç¡€

</summary>

- `âš¡` ğŸ“„ Neural Architecture Search with Reinforcement Learning
- `âš¡` ğŸ“„ Learning Transferable Architectures for Scalable Image Recognition

</details>

<details open>
<summary>

##### One-Shot NAS

</summary>

- `âš¡` ğŸ“„ DARTS: Differentiable Architecture Search

</details>

<details open>
<summary>

##### One-Shot NAS

</summary>

- `ğŸ’` ğŸ“„ (DARTS-PT) Rethinking Architecture Selection in Differentiable NAS
- `ğŸ’` ğŸ“„ Î²-DARTS: Beta-Decay Regularization for Differentiable Architecture Search

</details>

### æ¨¡å‹å‹ç¼©


<details open>
<summary>

##### Zero-Shot Proxy

</summary>

- `âš¡` ğŸ“„ Accelerating Neural Architecture Search via Proxy Data
- `âš¡` ğŸ“„ Zero-Cost Proxies MeetDifferentiable Architecture Search

</details>

<details open>
<summary>

##### æ¨¡å‹å‹ç¼©ç»¼è¿°

</summary>

> å¯¹æ¨¡å‹å‹ç¼©æ„Ÿå…´è¶£çš„åŒå­¦å¯ä»¥åœ¨è¿™ä¸€éƒ¨åˆ†åŸºç¡€ä¸Šè‡ªå·±æŸ¥è¯¢ç›¸å…³è®ºæ–‡é˜…è¯»

- `ğŸ’` ğŸ“„ Model Compression for Deep Neural Networks: A Survey

</details>

<details open>
<summary>

##### æ¨¡å‹å‰ªæåŸºç¡€

</summary>

- `ğŸ’` ğŸ“„ Pruning Filters for Efficient ConvNets
- `ğŸ’` ğŸ“„ Learning both weights and connections for efficient neural network

</details>

<details open>
<summary>

##### çŸ¥è¯†è’¸é¦åŸºç¡€

</summary>

- `ğŸ’` ğŸ“„ Distilling the Knowledge in a Neural Network
- `ğŸ’` ğŸ“„ Contrastive Representation Distillation

</details>

