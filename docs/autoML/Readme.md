<p align="center">
    <h1 align="center">â±ï¸ AutoMLæ–¹å‘</h1>
    <p align="center">å…¥é—¨æŒ‡å—</p>
    <p align="center">
        <a href="https://github.com/PKU-DAIR">
            <img alt="Static Badge" src="https://img.shields.io/badge/%C2%A9-PKU--DAIR-%230e529d?labelColor=%23003985">
        </a>
        <a href="https://github.com/PKU-DAIR">
            <img alt="Static Badge" src="https://img.shields.io/badge/PKU--DAIR-black?logo=github">
        </a>
    </p>
</p>

> æ³¨:âš¡ä¸º**åŸºç¡€å¿…è¯»**,ğŸ’ä¸º**åŸºç¡€é€‰è¯»**,ğŸ’¡ä¸º**è¿›é˜¶é˜…è¯»**

### AutoML ä¸è¶…å‚æ•°ä¼˜åŒ–


<details open>
<summary>

##### AutoMLç»¼è¿°

</summary>

- `âš¡` ğŸ“„ [AutoML: A Survey of the State-of-the-Art](https://www.sciencedirect.com/science/article/abs/pii/S0950705120307516)

</details>

<details open>
<summary>

##### è´å¶æ–¯ä¼˜åŒ–åŸºç¡€

</summary>

> å…ˆé€šè¿‡é˜…è¯»ä¸“æ äº†è§£è´å¶æ–¯ä¼˜åŒ–å’Œé«˜æ–¯è¿‡ç¨‹çš„åŸºæœ¬åŸç†ï¼Œç„¶åé˜…è¯»ä¸¤ç¯‡è®ºæ–‡äº†è§£SMBOçš„ç»å…¸æ–¹æ³•

- `âš¡` ğŸ“„ [(BO & GP)](https://zhuanlan.zhihu.com/p/139605200)
- `âš¡` ğŸ“„ (SMAC) [Sequential Model-Based Optimization for General Algorithm Configuration (extended version)](https://ai.dmi.unibas.ch/research/reading_group/hutter-et-al-tr2010.pdf)
- `âš¡` ğŸ“„ (TPE) [Algorithms for Hyper-Parameter Optimization](https://proceedings.neurips.cc/paper_files/paper/2011/hash/86e8f7ab32cfd12577bc2619bc635690-Abstract.html)

</details>

<details open>
<summary>

##### é»‘ç›’ä¼˜åŒ–ç³»ç»Ÿ

</summary>

> é€šè¿‡openboxè‡ªå·±æ­å»ºSMBOçš„åŸºæœ¬æµç¨‹
é˜…è¯»openboxæºä»£ç ï¼Œäº†è§£å…¶å†…éƒ¨å®ç°

- `âš¡` ğŸ› ï¸ [Openbox](https://github.com/PKU-DAIR/open-box)

</details>

<details open>
<summary>

##### å¤šç²¾åº¦ä¼˜åŒ–

</summary>

- `âš¡` ğŸ“„ [Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization](https://www.jmlr.org/papers/v18/16-558.html)
- `âš¡` ğŸ“„ [BOHB: Robust and Efficient Hyperparameter Optimization at Scale](https://proceedings.mlr.press/v80/falkner18a.html)

</details>

<details open>
<summary>

##### å¤šç²¾åº¦ä¼˜åŒ–

</summary>

- `ğŸ’` ğŸ“„ [DEHB: Evolutionary Hyperband for Scalable, Robust and Efficient Hyperparameter Optimization](https://www.ijcai.org/proceedings/2021/0296.pdf)
- `ğŸ’` ğŸ“„ [Efficient Automatic CASH via Rising Bandits](https://ojs.aaai.org/index.php/AAAI/article/view/5910)
- `ğŸ’` ğŸ“„ (DyHPO) [Supervising the Multi-Fidelity Race of Hyperparameter Configurations](https://proceedings.neurips.cc/paper_files/paper/2022/hash/57b694fef23ae7b9308eb4d46342595d-Abstract-Conference.html)

</details>

<details open>
<summary>

##### AutoMLç³»ç»Ÿ

</summary>

- `âš¡` ğŸ“„ [Auto-WEKA: combined selection and hyperparameter optimization of classification algorithms](https://dl.acm.org/doi/abs/10.1145/2487575.2487629)
- `âš¡` ğŸ“„ (auto-sklearn) [Efficient and Robust Automated Machine Learning](https://proceedings.neurips.cc/paper/2015/hash/11d0e6287202fced83f79975ec59a3a6-Abstract.html)

</details>

<details open>
<summary>

##### AutoMLç³»ç»Ÿ

</summary>

> é€šè¿‡å¼€æºautoMLç³»ç»Ÿè‡ªè¡Œæ­å»ºè‡ªåŠ¨åŒ–æœºå™¨å­¦ä¹ çš„å®Œæ•´æµç¨‹ï¼Œå¹¶äº†è§£å…¶å†…éƒ¨å®ç°

- `âš¡` ğŸ› ï¸ auto-sklearn:  https://github.com/automl/auto-sklearn
- `âš¡` ğŸ› ï¸ MindWare:  https://github.com/thomas-young-2013/mindware

</details>

<details open>
<summary>

##### è¿ç§»å­¦ä¹ ç»¼è¿°

</summary>

- `âš¡` ğŸ“„ [Transfer Learning for Bayesian Optimization: A Survey](https://arxiv.org/abs/2302.05927)

</details>

<details open>
<summary>

##### è¿ç§»å­¦ä¹ 

</summary>

- `âš¡` ğŸ“„ (RGPE) [Scalable Meta-Learning for Bayesian Optimization using Ranking-Weighted Gaussian Process Ensembles](https://aad.informatik.uni-freiburg.de/wp-content/uploads/papers/18-AUTOML-RGPE.pdf)
- `âš¡` ğŸ“„ (TAF) [Scalable Gaussian process-based transfer surrogates for hyperparameter optimization](https://link.springer.com/article/10.1007/s10994-017-5684-y)

- `ğŸ’` ğŸ“„ [Learning Hyperparameter Optimization Initializations](https://ieeexplore.ieee.org/abstract/document/7344817)
- `ğŸ’` ğŸ“„ [Hyperparameter Search Space Pruning â€“ A NewComponent for Sequential Model-Based Hyperparameter Optimization](https://link.springer.com/chapter/10.1007/978-3-319-23525-7_7)
- `ğŸ’` ğŸ“„ (POGOE) [Scalable Hyperparameter Optimization with Products of Gaussian Process Experts](https://link.springer.com/chapter/10.1007/978-3-319-46128-1_3)
- `ğŸ’` ğŸ“„ (TST) [Two-Stage Transfer Surrogate Model for Automatic Hyperparameter Optimization](https://link.springer.com/chapter/10.1007/978-3-319-46128-1_13)
- `ğŸ’` ğŸ“„ (TransBO) [TransBO: Hyperparameter Optimization via Two-Phase Transfer Learning](https://dl.acm.org/doi/abs/10.1145/3534678.3539255)

</details>

### ç½‘ç»œç»“æ„æœç´¢(NAS)

<details open>
<summary>

##### NASç»¼è¿°

</summary>

- `âš¡` ğŸ“„ [Neural Architecture Search: A Survey](https://www.jmlr.org/papers/v20/18-598.html)

</details>

<details open>
<summary>

##### NASåŸºç¡€

</summary>

- `âš¡` ğŸ“„ [Neural Architecture Search with Reinforcement Learning](https://arxiv.org/abs/1611.01578)
- `âš¡` ğŸ“„ [Learning Transferable Architectures for Scalable Image Recognition](https://openaccess.thecvf.com/content_cvpr_2018/html/Zoph_Learning_Transferable_Architectures_CVPR_2018_paper.html)

</details>

<details open>
<summary>

##### One-Shot NAS

</summary>

- `âš¡` ğŸ“„ [DARTS: Differentiable Architecture Search](https://arxiv.org/abs/1806.09055)

</details>

<details open>
<summary>

##### One-Shot NAS

</summary>

- `ğŸ’` ğŸ“„ (DARTS-PT) [Rethinking Architecture Selection in Differentiable NAS](https://arxiv.org/abs/2108.04392)
- `ğŸ’` ğŸ“„ [Î²-DARTS: Beta-Decay Regularization for Differentiable Architecture Search](https://openaccess.thecvf.com/content/CVPR2022/html/Ye_b-DARTS_Beta-Decay_Regularization_for_Differentiable_Architecture_Search_CVPR_2022_paper.html)

</details>

<details open>
<summary>

##### Zero-Shot Proxy

</summary>

- `âš¡` ğŸ“„ [Accelerating Neural Architecture Search via Proxy Data](https://arxiv.org/abs/2106.04784)
- `âš¡` ğŸ“„ [Zero-Cost Operation Scoring in Differentiable Architecture Search](https://arxiv.org/abs/2106.06799)
- `âš¡` ğŸ“„ [ProxyBO: Accelerating Neural Architecture Search via Bayesian Optimization with Zero-Cost Proxies](https://ojs.aaai.org/index.php/AAAI/article/view/26169)

</details>

### æ¨¡å‹å‹ç¼©

<details open>
<summary>

##### æ¨¡å‹å‹ç¼©ç»¼è¿°

</summary>

> å¯¹æ¨¡å‹å‹ç¼©æ„Ÿå…´è¶£çš„åŒå­¦å¯ä»¥åœ¨è¿™ä¸€éƒ¨åˆ†åŸºç¡€ä¸Šè‡ªå·±æŸ¥è¯¢ç›¸å…³è®ºæ–‡é˜…è¯»

- `ğŸ’` ğŸ“„ [Model Compression for Deep Neural Networks: A Survey](https://www.mdpi.com/2073-431X/12/3/60)

</details>

<details open>
<summary>

##### æ¨¡å‹å‰ªæåŸºç¡€

</summary>

- `ğŸ’` ğŸ“„ [Pruning Filters for Efficient ConvNets](https://arxiv.org/abs/1608.08710)
- `ğŸ’` ğŸ“„ [Learning both weights and connections for efficient neural network](https://proceedings.neurips.cc/paper/2015/hash/ae0eb3eed39d2bcef4622b2499a05fe6-Abstract.html)

</details>

<details open>
<summary>

##### çŸ¥è¯†è’¸é¦åŸºç¡€

</summary>

- `ğŸ’` ğŸ“„ [Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531)
- `ğŸ’` ğŸ“„ [Contrastive Representation Distillation]()https://arxiv.org/abs/1910.10699

</details>

<details open>
<summary>

##### å‚æ•°é‡åŒ–åŸºç¡€

</summary>

- `ğŸ’` ğŸ“„ [ZeroQ: A Novel Zero Shot Quantization Framework](https://openaccess.thecvf.com/content_CVPR_2020/html/Cai_ZeroQ_A_Novel_Zero_Shot_Quantization_Framework_CVPR_2020_paper.html)
- `ğŸ’` ğŸ“„ [Post-Training Sparsity-Aware Quantization](https://proceedings.neurips.cc/paper/2021/hash/9431c87f273e507e6040fcb07dcb4509-Abstract.html)

</details>

### å¤§è¯­è¨€æ¨¡å‹ä¸AutoML

<details open>
<summary>

##### LLM for AutoML

</summary>

> ä½¿ç”¨å¤§æ¨¡å‹å¸®åŠ©AutoMLçš„æµç¨‹æ‰§è¡Œ

- `ğŸ’` ğŸ“„ [Large Language Models to Enhance Bayesian Optimization](https://arxiv.org/abs/2402.03921)
- `ğŸ’` ğŸ“„ [Large Language Models for Automated Data Science: Introducing CAAFE for Context-Aware Automated Feature Engineering](https://proceedings.neurips.cc/paper_files/paper/2023/hash/8c2df4c35cdbee764ebb9e9d0acd5197-Abstract-Conference.html)

</details>