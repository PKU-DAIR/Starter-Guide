<p align="center">
    <h1 align="center">ğŸ’» ç³»ç»Ÿæ–¹å‘</h1>
    <p align="center">å…¥é—¨æŒ‡å—</p>
    <p align="center">
        <a href="https://github.com/PKU-DAIR">
            <img alt="Static Badge" src="https://img.shields.io/badge/%C2%A9-PKU--DAIR-%230e529d?labelColor=%23003985">
        </a>
        <a href="https://github.com/PKU-DAIR">
            <img alt="Static Badge" src="https://img.shields.io/badge/PKU--DAIR-black?logo=github">
        </a>
    </p>
</p>

### ML/DLç³»ç»Ÿæ¡†æ¶

> æ³¨:âš¡ä¸º**åŸºç¡€å¿…è¯»**,ğŸ’ä¸º**åŸºç¡€é€‰è¯»**,ğŸ’¡ä¸º**è¿›é˜¶é˜…è¯»**

<details open>
<summary>

##### ML/DLç³»ç»ŸåŸºç¡€

</summary>

**âš¡ åŸºç¡€å¿…è¯»**

- ğŸ“ [CMU DL Systems è¯¾ç¨‹](https://dlsyscourse.org/)

> å­¦ä¹ çº¿ä¸Šè¯¾ç¨‹å¹¶å®Œæˆä½œä¸šï¼Œäº†è§£æ·±åº¦å­¦ä¹ ç³»ç»Ÿçš„åŸºæœ¬ç»„æˆï¼ŒåŒæ—¶è¦æ±‚ä½œä¸šä¸èƒ½å…‰åšå®Œå°±ç®—äº†ï¼Œæ³¨é‡è¿è¡Œæ•ˆç‡çš„ä¼˜åŒ–ï¼Œæ¯”å¦‚è·ŸPyTorchå¯¹æ¯”ï¼Œæ˜¯æ›´å¿«è¿˜æ˜¯æ›´æ…¢

**ğŸ’åŸºç¡€é€‰è¯»**

- ğŸ“ [System for AI](https://github.com/microsoft/AI-System)
</details>

<details open>
<summary>

##### DLç³»ç»Ÿæ¡†æ¶âš¡

</summary>

- ğŸ› ï¸ [PyTorch](https://dlsyscourse.org/)
</details>

### åˆ†å¸ƒå¼è®­ç»ƒ

<details open>
<summary>

##### åˆ†å¸ƒå¼è®­ç»ƒæ¡†æ¶âš¡

</summary>

> è°ƒç ”å­¦ä¹ ç°åœ¨ä¸»æµçš„åˆ†å¸ƒå¼è®­ç»ƒç³»ç»Ÿï¼ŒåŒ…æ‹¬ä»–ä»¬çš„ç³»ç»Ÿæ¡†æ¶å®ç°ï¼Œä»¥åŠä»‹ç»ä¸»è¦æŠ€æœ¯çš„æ–‡ç« 

- ğŸ› ï¸ [Megatron-LM](https://github.com/NVIDIA/Megatron-LM)
- ğŸ› ï¸ [DeepSpeed](https://github.com/microsoft/DeepSpeed)
</details>

<details open>
<summary>

##### åˆ†å¸ƒå¼è®­ç»ƒç»¼è¿°âš¡

</summary>

- ğŸ“„ Demystifying Parallel and Distributed Deep Learning: An In-Depth Concurrency Analysis
- ğŸ“„ Efficient Training of Large Language Models on Distributed Infrastructures: A Survey
</details>

<details open>
<summary>

##### æ•°æ®å¹¶è¡Œ(All-Reduce based)âš¡

</summary>

- ğŸ“„ **Horovod** | Fast and easy distributed deep learning in TensorFlow
- ğŸ“„ **PyTorch DDP** | PyTorch Distributed: Experiences on Accelerating Data Parallel Training
</details>

<details open>
<summary>

##### ZeRO/FSDPå¹¶è¡Œâš¡

</summary>

- ğŸ“„ ZeRO: memory optimizations toward training trillion parameter models
- ğŸ“„ ZeRO-Offload: Democratizing Billion-Scale Model Training
</details>

<details open>
<summary>

##### æµæ°´å¹¶è¡Œâš¡

</summary>

- ğŸ“„ GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism
- ğŸ“„ PipeDream: Fast and Efficient Pipeline Parallel DNN Training
- ğŸ“„ **PipeDream-2BW/PipeDream-Flush/1F1B-Flush** | Memory-Efficient Pipeline-Parallel DNN Training
</details>

<details open>
<summary>

##### å¼ é‡å¹¶è¡Œ/ç®—å­å¹¶è¡Œâš¡

</summary>

- ğŸ“„ [OWT] One weird trick for parallelizing convolutional neural networks
- ğŸ“„ [Megatron-style TP] Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism
</details>

<details open>
<summary>

##### 3Då¹¶è¡Œâš¡

</summary>

- ğŸ“„ Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM
</details>

<details open>
<summary>

##### è‡ªåŠ¨å¹¶è¡Œâš¡

</summary>

- ğŸ“„ [FlexFlow] Beyond Data and Model Parallelism for Deep Neural Networks
- ğŸ“„ Galvatron: Efficient Transformer Training over Multiple GPUs Using Automatic Parallelism
- ğŸ“„ Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning
</details>

<details open>
<summary>

##### Tensor AnnotationğŸ’

</summary>

- ğŸ“„ GSPMD: General and Scalable Parallelization for ML Computation Graphs
- ğŸ“„ Unity: Accelerating DNN Training Through Joint Optimization of Algebraic Transformations and Parallelization
</details>

<details open>
<summary>

##### ä¸“å®¶å¹¶è¡Œ/MoEè®­ç»ƒğŸ’

</summary>

- ğŸ“„ GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding
- ğŸ“„ A Hybrid Tensor-Expert-Data Parallelism Approach to Optimize Mixture-of-Experts Training
- ğŸ“„ FasterMoE:Â modeling and optimizing training of large-scale dynamic pre-trained models
</details>

<details open>
<summary>

##### åºåˆ—å¹¶è¡Œ/é•¿çª—å£å¹¶è¡ŒğŸ’

</summary>

- ğŸ“„ [Megatron-style SP] Reducing Activation Recomputation in Large Transformer Models
- ğŸ“„ [Ulyssys-style SP] DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models
- ğŸ“„ [RSA] Sequence parallelism: Long sequence training from system perspective
- ğŸ“„ [Ring-Attn] Ring Attention with Blockwise Transformers for Near-Infinite Context
- ğŸ“„ [LightSeq/DistFlashAttn] DISTFLASHATTN: Distributed Memory-efficient Attention for Long-context LLMs Training
</details>

<details open>
<summary>

##### æ˜¾å­˜èŠ‚çº¦ğŸ’

</summary>

- ğŸ“„ [Activation Checkpointing/Recomputation] Training Deep Nets with Sublinear Memory Cost
- ğŸ“„ [Offloading] vDNN: Virtualized Deep Neural Networks for Scalable, Memory-Efficient Neural Network Design
</details>

<details open>
<summary>

##### ç®—å­èåˆä¼˜åŒ–ğŸ’

</summary>

- ğŸ“„ FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness
- ğŸ“„ FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning
</details>

<details open>
<summary>

##### é•¿åºåˆ—è®­ç»ƒğŸ’¡

</summary>

- ğŸ“„ Striped Attention: Faster Ring Attention for Causal Transformers
- ğŸ“„ USP: A Unified Sequence Parallelism Approach for Long Context Generative AI
- ğŸ“„ LoongTrain: Efficient Training of Long-Sequence LLMs with Head-Context Parallelism
</details>

<details open>
<summary>

##### å¤šæ¨¡æ€è®­ç»ƒğŸ’¡

</summary>

- ğŸ“„ DistTrain: Addressing Model and Data Heterogeneity with Disaggregated Training for Multimodal Large Language Models
- ğŸ“„ Optimus: Accelerating Large-Scale Multi-Modal LLM Training by Bubble Exploitation
- ğŸ“„ Efficient Multi-Task Large Model Training via Data Heterogeneity-aware Model Management
</details>

<details open>
<summary>

##### å¼‚æ„è®­ç»ƒğŸ’¡

</summary>

- ğŸ“„ HetHub: A Heterogeneous distributed hybrid training system for large-scale models
- ğŸ“„ AMP: Automatically Finding Model Parallel Strategies with Heterogeneity Awareness
- ğŸ“„ AccPar: Tensor Partitioning for Heterogeneous Deep Learning Accelerators
- ğŸ“„ Whale: Scaling Deep Learning Model Training to the Trillions
</details>

### LLMæ¨ç†æœåŠ¡
<details open>
<summary>

##### LLMæœåŠ¡ç³»ç»Ÿæ¡†æ¶**
</summary>

- ğŸ› ï¸ [vLLM](https://github.com/vllm-project/vllm)
</details>

<details open>
<summary>

##### ç»¼è¿°âš¡
</summary>

- ğŸ“„ A Survey on Efficient Inference for Large Language Models
- ğŸ“„ Towards Efficient Generative Large Language Model Serving: A Survey from Algorithms to Systems
</details>

<details open>
<summary>

##### Batchingâš¡
</summary>

- ğŸ“„ Orca: A Distributed Serving System for Transformer-Based Generative Models
</details>

<details open>
<summary>

##### æ¨¡å‹å¹¶è¡Œâš¡
</summary>

- ğŸ“„ AlpaServe: Statistical Multiplexing with Model Parallelism for Deep Learning Serving
</details>

<details open>
<summary>

##### æ˜¾å­˜ç®¡ç†âš¡
</summary>

- ğŸ“„ Efficient Memory Management for Large Language Model Serving with PagedAttention
</details>

<details open>
<summary>

##### æŠ•æœºæ¨ç†âš¡
</summary>

- ğŸ“„ Fast Inference from Transformers via Speculative Decoding
</details>

<details open>
<summary>

##### Prefix Sharingâš¡
<summary>

- ğŸ“„ SGLang: Efficient Execution of Structured Language Model Programs
</details>

<details open>
<summary>

##### PDåˆ†ç¦»ğŸ’
</summary>

- ğŸ“„ DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving
- ğŸ“„ Splitwise: Efficient generative LLM inference using phase splitting
- ğŸ“„ Inference without Interference: Disaggregate LLM Inference for Mixed Downstream Workloads
- ğŸ“„ Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving
</details>

<details open>
<summary>

##### Chunked PrefillğŸ’
</summary>

- ğŸ“„ Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve
</details>

<details open>
<summary>

##### æ¨¡å‹å‹ç¼©ğŸ’
</summary>

- ğŸ“„ GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers
- ğŸ“„ AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration
</details>

### Diffusionï¼ˆæ–‡ç”Ÿå›¾ã€æ–‡ç”Ÿè§†é¢‘ï¼‰æ¨ç†æœåŠ¡