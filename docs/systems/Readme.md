<p align="center">
    <h1 align="center">ğŸ’» ç³»ç»Ÿæ–¹å‘</h1>
    <p align="center">å…¥é—¨æŒ‡å—</p>
    <p align="center">
        <a href="https://github.com/PKU-DAIR">
            <img alt="Static Badge" src="https://img.shields.io/badge/%C2%A9-PKU--DAIR-%230e529d?labelColor=%23003985">
        </a>
        <a href="https://github.com/PKU-DAIR">
            <img alt="Static Badge" src="https://img.shields.io/badge/PKU--DAIR-black?logo=github">
        </a>
    </p>
</p>

> æ³¨:âš¡ä¸º**åŸºç¡€å¿…è¯»**,ğŸ’ä¸º**åŸºç¡€é€‰è¯»**,ğŸ’¡ä¸º**è¿›é˜¶é˜…è¯»**

### ML/DLç³»ç»Ÿæ¡†æ¶


<details open>
<summary>

##### ML/DLç³»ç»ŸåŸºç¡€

</summary>

> å­¦ä¹ çº¿ä¸Šè¯¾ç¨‹å¹¶å®Œæˆä½œä¸šï¼Œäº†è§£æ·±åº¦å­¦ä¹ ç³»ç»Ÿçš„åŸºæœ¬ç»„æˆï¼ŒåŒæ—¶è¦æ±‚ä½œä¸šä¸èƒ½å…‰åšå®Œå°±ç®—äº†ï¼Œæ³¨é‡è¿è¡Œæ•ˆç‡çš„ä¼˜åŒ–ï¼Œæ¯”å¦‚è·ŸPyTorchå¯¹æ¯”ï¼Œæ˜¯æ›´å¿«è¿˜æ˜¯æ›´æ…¢

- `âš¡` ğŸ“ CMU DL Systemsè¯¾ç¨‹ï¼Œhttps://dlsyscourse.org/

</details>

<details open>
<summary>

##### ML/DLç³»ç»ŸåŸºç¡€

</summary>

- `ğŸ’` ğŸ“ System for AIï¼Œhttps://github.com/microsoft/AI-System

</details>

<details open>
<summary>

##### DLç³»ç»Ÿæ¡†æ¶

</summary>

- `âš¡` ğŸ› ï¸ [PyTorch](https://pytorch.org/)

</details>

### åˆ†å¸ƒå¼è®­ç»ƒ

<details open>
<summary>

##### ï¼ˆåˆ†å¸ƒå¼ï¼‰è®­ç»ƒæ¡†æ¶

</summary>

> è°ƒç ”å­¦ä¹ ç°åœ¨ä¸»æµçš„åˆ†å¸ƒå¼è®­ç»ƒç³»ç»Ÿï¼ŒåŒ…æ‹¬ä»–ä»¬çš„ç³»ç»Ÿæ¡†æ¶å®ç°ï¼Œä»¥åŠä»‹ç»ä¸»è¦æŠ€æœ¯çš„æ–‡ç« 

- `âš¡` ğŸ› ï¸ Megatron-LMï¼Œhttps://github.com/NVIDIA/Megatron-LM
- `âš¡` ğŸ› ï¸ DeepSpeedï¼Œhttps://github.com/microsoft/DeepSpeed

</details>

<details open>
<summary>

##### åˆ†å¸ƒå¼è®­ç»ƒç»¼è¿°

</summary>

> ä»‹ç»åˆ†å¸ƒå¼è®­ç»ƒçš„blog postæœ‰å¾ˆå¤šï¼Œå¯ä»¥ä¸Šç½‘æœç´¢å…¶ä»–ç›¸å…³å†…å®¹è¿›è¡Œåˆæ­¥äº†è§£ï¼Œæœ‰ä¸€å®šåŸºç¡€çŸ¥è¯†å‚¨å¤‡åå†çœ‹è¿™ä¸¤ç¯‡ç»¼è¿°ä¼šæ›´åˆé€‚

- `ğŸ’` ğŸ“„ Demystifying Parallel and Distributed Deep Learning: An In-Depth Concurrency Analysis
- `ğŸ’` ğŸ“„ Efficient Training of Large Language Models on Distributed Infrastructures: A Survey

</details>

<details open>
<summary>

##### æ•°æ®å¹¶è¡Œï¼ˆAll-Reduce basedï¼‰

</summary>

- `âš¡` ğŸ“„ [Horovod] Fast and easy distributed deep learning in TensorFlow
- `âš¡` ğŸ“„ [PyTorch DDP] PyTorch Distributed: Experiences on Accelerating Data Parallel Training

</details>

<details open>
<summary>

##### ZeRO/FSDPå¹¶è¡Œ

</summary>

- `âš¡` ğŸ“„ ZeRO: memory optimizations toward training trillion parameter models
- `âš¡` ğŸ“„ ZeRO-Offload: Democratizing Billion-Scale Model Training

</details>

<details open>
<summary>

##### æµæ°´å¹¶è¡Œ

</summary>

- `âš¡` ğŸ“„ GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism
- `âš¡` ğŸ“„ PipeDream: Fast and Efficient Pipeline Parallel DNN Training
- `âš¡` ğŸ“„ [PipeDream-2BW/PipeDream-Flush/1F1B-Flush] Memory-Efficient Pipeline-Parallel DNN Training

</details>

<details open>
<summary>

##### å¼ é‡å¹¶è¡Œ/ç®—å­å¹¶è¡Œ

</summary>

- `âš¡` ğŸ“„ [OWT] One weird trick for parallelizing convolutional neural networks
- `âš¡` ğŸ“„ [Megatron-style TP] Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism

</details>

<details open>
<summary>

##### 3Då¹¶è¡Œ

</summary>

- `âš¡` ğŸ“„ Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM

</details>

<details open>
<summary>

##### è‡ªåŠ¨å¹¶è¡Œ

</summary>

- `âš¡` ğŸ“„ [FlexFlow] Beyond Data and Model Parallelism for Deep Neural Networks
- `âš¡` ğŸ“„ Galvatron: Efficient Transformer Training over Multiple GPUs Using Automatic Parallelism
- `âš¡` ğŸ“„ Alpa: Automating Interand Intra-Operator Parallelism for Distributed Deep Learning

</details>

<details open>
<summary>

##### Tensor Annotation

</summary>

- `ğŸ’` ğŸ“„ GSPMD: General and Scalable Parallelization for ML Computation Graphs
- `ğŸ’` ğŸ“„ Unity: Accelerating DNN Training Through Joint Optimization of Algebraic Transformations and Parallelization

</details>

<details open>
<summary>

##### ä¸“å®¶å¹¶è¡Œ/MoEè®­ç»ƒ

</summary>

- `ğŸ’` ğŸ“„ GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding
- `ğŸ’` ğŸ“„ A Hybrid Tensor-Expert-Data Parallelism Approach to Optimize Mixture-of-Experts Training
- `ğŸ’` ğŸ“„ FasterMoE:Â modeling and optimizing training of large-scale dynamic pre-trained models

</details>

<details open>
<summary>

##### åºåˆ—å¹¶è¡Œ/é•¿çª—å£å¹¶è¡Œ

</summary>

- `ğŸ’` ğŸ“„ [Megatron-style SP] Reducing Activation Recomputation in Large Transformer Models
- `ğŸ’` ğŸ“„ [Ulyssys-style SP] DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models
- `ğŸ’` ğŸ“„ [RSA] Sequence parallelism: Long sequence training from system perspective
- `ğŸ’` ğŸ“„ [Ring-Attn] Ring Attention with Blockwise Transformers for Near-Infinite Context
- `ğŸ’` ğŸ“„ [LightSeq/DistFlashAttn] DISTFLASHATTN: Distributed Memory-efficient Attention for Long-context LLMs Training

</details>

<details open>
<summary>

##### æ˜¾å­˜èŠ‚çº¦

</summary>

- `ğŸ’` ğŸ“„ [Activation Checkpointing/Recomputation] Training Deep Nets with Sublinear Memory Cost
- `ğŸ’` ğŸ“„ [Offloading] vDNN: Virtualized Deep Neural Networks for Scalable, Memory-Efficient Neural Network Design

</details>

<details open>
<summary>

##### ç®—å­èåˆä¼˜åŒ–

</summary>

- `ğŸ’` ğŸ“„ FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness
- `ğŸ’` ğŸ“„ FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning

</details>

<details open>
<summary>

##### é•¿åºåˆ—è®­ç»ƒ

</summary>

- `ğŸ’¡` ğŸ“„ Striped Attention: Faster Ring Attention for Causal Transformers
- `ğŸ’¡` ğŸ“„ USP: A Unified Sequence Parallelism Approach for Long Context Generative AI
- `ğŸ’¡` ğŸ“„ LoongTrain: Efficient Training of Long-Sequence LLMs with Head-Context Parallelism

</details>

<details open>
<summary>

##### å¤šæ¨¡æ€è®­ç»ƒ

</summary>

- `ğŸ’¡` ğŸ“„ DistTrain: Addressing Model and Data Heterogeneity with Disaggregated Training for Multimodal Large Language Models
- `ğŸ’¡` ğŸ“„ Optimus: Accelerating Large-Scale Multi-Modal LLM Training by Bubble Exploitation
- `ğŸ’¡` ğŸ“„ Efficient Multi-Task Large Model Training via Data Heterogeneity-aware Model Management

</details>

<details open>
<summary>

##### å¼‚æ„è®­ç»ƒ

</summary>

- `ğŸ’¡` ğŸ“„ HetHub: A Heterogeneous distributed hybrid training system for large-scale models
- `ğŸ’¡` ğŸ“„ AMP: Automatically Finding Model Parallel Strategies with Heterogeneity Awareness
- `ğŸ’¡` ğŸ“„ AccPar: Tensor Partitioning for Heterogeneous Deep Learning Accelerators
- `ğŸ’¡` ğŸ“„ Whale: Scaling Deep Learning Model Training to the Trillions

</details>

### LLMæ¨ç†æœåŠ¡

<details open>
<summary>

##### LLMæœåŠ¡ç³»ç»Ÿæ¡†æ¶

</summary>

- `âš¡` ğŸ› ï¸ vLLMï¼Œhttps://github.com/vllm-project/vllm

</details>

<details open>
<summary>

##### ç»¼è¿°

</summary>

- `âš¡` ğŸ“„ A Survey on Efficient Inference for Large Language Models
- `âš¡` ğŸ“„ Towards Efficient Generative Large Language Model Serving: A Survey from Algorithms to Systems

</details>

<details open>
<summary>

##### Batching

</summary>

- `âš¡` ğŸ“„ Orca: A Distributed Serving System for Transformer-Based Generative Models

</details>

<details open>
<summary>

##### æ¨¡å‹å¹¶è¡Œ

</summary>

> åˆ†å¸ƒå¼æ¨ç†ä¸­ä¹Ÿæœ‰ç”¨åˆ°å¾ˆå¤šå¹¶è¡Œç­–ç•¥ï¼Œå»ºè®®å¯¹å‰é¢å…³äºå¹¶è¡Œç­–ç•¥çš„å†…å®¹ä¹Ÿè¿›è¡Œé˜…è¯»äº†è§£

- `âš¡` ğŸ“„ AlpaServe: Statistical Multiplexing with Model Parallelism for Deep Learning Serving

</details>

<details open>
<summary>

##### æ˜¾å­˜ç®¡ç†

</summary>

- `âš¡` ğŸ“„ Efficient Memory Management for Large Language Model Serving with PagedAttention

</details>

<details open>
<summary>

##### æŠ•æœºæ¨ç†

</summary>

- `âš¡` ğŸ“„ Fast Inference from Transformers via Speculative Decoding

</details>

<details open>
<summary>

##### Prefix Sharing

</summary>

- `âš¡` ğŸ“„ SGLang: Efficient Execution of Structured Language Model Programs

</details>

<details open>
<summary>

##### PDåˆ†ç¦»

</summary>

- `ğŸ’` ğŸ“„ DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving
- `ğŸ’` ğŸ“„ Splitwise: Efficient generative LLM inference using phase splitting
- `ğŸ’` ğŸ“„ Inference without Interference: Disaggregate LLM Inference for Mixed Downstream Workloads
- `ğŸ’` ğŸ“„ Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving

</details>

<details open>
<summary>

##### Chunked Prefill

</summary>

- `ğŸ’` ğŸ“„ Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve

</details>

<details open>
<summary>

##### æ¨¡å‹å‹ç¼©

</summary>

- `ğŸ’` ğŸ“„ GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers
- `ğŸ’` ğŸ“„ AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration

</details>

### Diffusionï¼ˆæ–‡ç”Ÿå›¾ã€æ–‡ç”Ÿè§†é¢‘ï¼‰æ¨ç†æœåŠ¡

<details open>
<summary>

##### DiTç³»ç»Ÿæ¡†æ¶

</summary>

- `âš¡` ğŸ› ï¸ xDiTï¼Œhttps://github.com/xdit-project/xDiT
- `âš¡` ğŸ› ï¸ VideoSysï¼Œhttps://github.com/NUS-HPC-AI-Lab/VideoSys

</details>

##### åˆ†å¸ƒå¼æ¨ç†

</summary>

- `âš¡` ğŸ“„ DistriFusion: Distributed Parallel Inference for High-Resolution Diffusion Models
- `âš¡` ğŸ“„ PipeFusion: Displaced Patch Pipeline Parallelism for Inference of Diffusion Transformer Models
</details>

