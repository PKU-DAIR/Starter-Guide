<p align="center">
    <h1 align="center">ğŸ’» AIç³»ç»Ÿæ–¹å‘</h1>
    <p align="center">å…¥é—¨æŒ‡å—</p>
    <p align="center">
        <a href="https://github.com/PKU-DAIR">
            <img alt="Static Badge" src="https://img.shields.io/badge/%C2%A9-PKU--DAIR-%230e529d?labelColor=%23003985">
        </a>
        <a href="https://github.com/PKU-DAIR">
            <img alt="Static Badge" src="https://img.shields.io/badge/PKU--DAIR-black?logo=github">
        </a>
    </p>
</p>

> æ³¨:âš¡ä¸º**åŸºç¡€å¿…è¯»**,ğŸ’ä¸º**åŸºç¡€é€‰è¯»**,ğŸ’¡ä¸º**è¿›é˜¶é˜…è¯»**

### AIåŸºç¡€å…¥é—¨

<details open>
<summary>

##### ML/DLã€AIGCå¤§æ¨¡å‹åŸºç¡€

</summary>

> é¢å‘ä¹‹å‰æ²¡æœ‰ ML/DLã€AIGCå¤§æ¨¡å‹ ç›¸å…³åŸºç¡€çš„åŒå­¦

- `âš¡` ğŸ“” [æå®æ¯…æœºå™¨å­¦ä¹ ](https://www.bilibili.com/video/BV1Wv411h7kN/?share_source=copy_web&vd_source=f5a7a6bc8935280a6ac3bbea7f7740eb)
- `âš¡` ğŸ“” [åŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹ ](https://zh.d2l.ai/)
- `âš¡` ğŸ“¦ [å¤æ—¦å¤§å­¦LLMæ•™æ](https://intro-llm.github.io/chapter/LLM-TAP.pdf)

</details>

### ML/DLç³»ç»Ÿæ¡†æ¶

<details open>
<summary>

##### ML/DLç³»ç»ŸåŸºç¡€

</summary>

> å­¦ä¹ çº¿ä¸Šè¯¾ç¨‹å¹¶å®Œæˆä½œä¸šï¼Œäº†è§£æ·±åº¦å­¦ä¹ ç³»ç»Ÿçš„åŸºæœ¬ç»„æˆï¼ŒåŒæ—¶è¦æ±‚ä½œä¸šä¸èƒ½å…‰åšå®Œå°±ç®—äº†ï¼Œæ³¨é‡è¿è¡Œæ•ˆç‡çš„ä¼˜åŒ–ï¼Œæ¯”å¦‚è·ŸPyTorchå¯¹æ¯”ï¼Œæ˜¯æ›´å¿«è¿˜æ˜¯æ›´æ…¢

- `âš¡` ğŸ“ [CMU DL Systemsè¯¾ç¨‹](https://dlsyscourse.org/)
- `ğŸ’` ğŸ“ [System for AI](https://github.com/microsoft/AI-System)

</details>

<details open>
<summary>

##### DLç³»ç»Ÿæ¡†æ¶

</summary>

- `âš¡` ğŸ› ï¸ [PyTorch](https://pytorch.org/)
- `ğŸ’¡` ğŸ› ï¸ [Jax](https://jax.readthedocs.io/en/latest/)
- `ğŸ’¡` ğŸ› ï¸ [Hetu](https://github.com/PKU-DAIR/Hetu)

</details>

### åˆ†å¸ƒå¼è®­ç»ƒ

<details open>
<summary>

##### ï¼ˆåˆ†å¸ƒå¼ï¼‰è®­ç»ƒæ¡†æ¶

</summary>

> è°ƒç ”å­¦ä¹ ç°åœ¨ä¸»æµçš„åˆ†å¸ƒå¼è®­ç»ƒç³»ç»Ÿï¼ŒåŒ…æ‹¬ä»–ä»¬çš„ç³»ç»Ÿæ¡†æ¶å®ç°ï¼Œä»¥åŠä»‹ç»ä¸»è¦æŠ€æœ¯çš„æ–‡ç« 

- `âš¡` ğŸ› ï¸ [Megatron-LM](https://github.com/NVIDIA/Megatron-LM)
- `âš¡` ğŸ› ï¸ [DeepSpeed](https://github.com/microsoft/DeepSpeed)
- `ğŸ’¡` ğŸ› ï¸ [Galvatron](https://github.com/PKU-DAIR/Hetu-Galvatron)

</details>

<details open>
<summary>

##### åˆ†å¸ƒå¼è®­ç»ƒç»¼è¿°

</summary>

> ä»‹ç»åˆ†å¸ƒå¼è®­ç»ƒçš„blog postæœ‰å¾ˆå¤šï¼Œå¯ä»¥ä¸Šç½‘æœç´¢å…¶ä»–ç›¸å…³å†…å®¹è¿›è¡Œåˆæ­¥äº†è§£ï¼Œæœ‰ä¸€å®šåŸºç¡€çŸ¥è¯†å‚¨å¤‡åå†çœ‹è¿™ä¸¤ç¯‡ç»¼è¿°ä¼šæ›´åˆé€‚

- `ğŸ’` ğŸ“„ [Demystifying Parallel and Distributed Deep Learning: An In-Depth Concurrency Analysis](https://arxiv.org/abs/1802.09941)
- `ğŸ’` ğŸ“„ [Efficient Training of Large Language Models on Distributed Infrastructures: A Survey](https://arxiv.org/abs/2407.20018)

</details>

<details open>
<summary>

##### æ•°æ®å¹¶è¡Œï¼ˆAll-Reduce basedï¼‰

</summary>

- `âš¡` ğŸ“„ [(Horovod) Fast and easy distributed deep learning in TensorFlow](https://arxiv.org/abs/1802.05799)
- `âš¡` ğŸ“„ [(PyTorch DDP) PyTorch Distributed: Experiences on Accelerating Data Parallel Training](https://www.vldb.org/pvldb/vol13/p3005-li.pdf)

</details>

<details open>
<summary>

##### ZeRO/FSDPå¹¶è¡Œ

</summary>

- `âš¡` ğŸ“„ [ZeRO: memory optimizations toward training trillion parameter models](https://dl.acm.org/doi/10.5555/3433701.3433727)
- `âš¡` ğŸ“„ [PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel](https://www.vldb.org/pvldb/vol16/p3848-huang.pdf)
- `âš¡` ğŸ“„ [ZeRO-Offload: Democratizing Billion-Scale Model Training](https://www.usenix.org/system/files/atc21-ren-jie.pdf)

</details>

<details open>
<summary>

##### æµæ°´å¹¶è¡Œ

</summary>

- `âš¡` ğŸ“„ [GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism](https://dl.acm.org/doi/abs/10.5555/3454287.3454297)
- `âš¡` ğŸ“„ [PipeDream: Fast and Efficient Pipeline Parallel DNN Training](https://dl.acm.org/doi/10.1145/3341301.3359646)
- `âš¡` ğŸ“„ [(PipeDream-2BW/PipeDream-Flush/1F1B-Flush) Memory-Efficient Pipeline-Parallel DNN Training](https://proceedings.mlr.press/v139/narayanan21a/narayanan21a.pdf)

</details>

<details open>
<summary>

##### å¼ é‡å¹¶è¡Œ/ç®—å­å¹¶è¡Œ

</summary>

- `âš¡` ğŸ“„ [(OWT) One weird trick for parallelizing convolutional neural networks](https://arxiv.org/abs/1404.5997)
- `âš¡` ğŸ“„ [(Megatron-style TP) Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/abs/1909.08053)

</details>

<details open>
<summary>

##### 3Då¹¶è¡Œ

</summary>

- `âš¡` ğŸ“„ [Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://dl.acm.org/doi/10.1145/3458817.3476209)

</details>

<details open>
<summary>

##### è‡ªåŠ¨å¹¶è¡Œ

</summary>

- `âš¡` ğŸ“„ [(FlexFlow) Beyond Data and Model Parallelism for Deep Neural Networks](https://proceedings.mlsys.org/paper_files/paper/2019/file/b422680f3db0986ddd7f8f126baaf0fa-Paper.pdf)
- `âš¡` ğŸ“„ [Galvatron: Efficient Transformer Training over Multiple GPUs Using Automatic Parallelism](https://www.vldb.org/pvldb/vol16/p470-miao.pdf)
- `âš¡` ğŸ“„ [Alpa: Automating Interand Intra-Operator Parallelism for Distributed Deep Learning](https://www.usenix.org/system/files/osdi22-zheng-lianmin.pdf)

</details>

<details open>
<summary>

##### Tensor Annotation

</summary>

- `ğŸ’` ğŸ“„ [GSPMD: General and Scalable Parallelization for ML Computation Graphs](https://arxiv.org/abs/2105.04663)
- `ğŸ’` ğŸ“„ [Unity: Accelerating DNN Training Through Joint Optimization of Algebraic Transformations and Parallelization](https://www.usenix.org/system/files/osdi22-unger.pdf)

</details>

<details open>
<summary>

##### ä¸“å®¶å¹¶è¡Œ/MoEè®­ç»ƒ

</summary>

- `ğŸ’` ğŸ“„ [GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding](https://arxiv.org/abs/2006.16668)
- `ğŸ’` ğŸ“„ [A Hybrid Tensor-Expert-Data Parallelism Approach to Optimize Mixture-of-Experts Training](https://dl.acm.org/doi/10.1145/3577193.3593704)
- `ğŸ’` ğŸ“„ [FasterMoE:Â modeling and optimizing training of large-scale dynamic pre-trained models](https://dl.acm.org/doi/10.1145/3503221.3508418)
- `ğŸ’` ğŸ“„ [FlexMoE: Scaling Large-scale Sparse Pre-trained Model Training via Dynamic Device Placement](https://dl.acm.org/doi/10.1145/3588964)

</details>

<details open>
<summary>

##### åºåˆ—å¹¶è¡Œ/é•¿çª—å£å¹¶è¡Œ

</summary>

- `ğŸ’` ğŸ“„ [(Megatron-style SP) Reducing Activation Recomputation in Large Transformer Models](https://proceedings.mlsys.org/paper_files/paper/2023/file/80083951326cf5b35e5100260d64ed81-Paper-mlsys2023.pdf)
- `ğŸ’` ğŸ“„ [(Ulyssys-style SP) DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models](https://arxiv.org/abs/2309.14509)
- `ğŸ’` ğŸ“„ [(RSA) Sequence parallelism: Long sequence training from system perspective](https://aclanthology.org/2023.acl-long.134.pdf)
- `ğŸ’` ğŸ“„ [(Ring-Attn) Ring Attention with Blockwise Transformers for Near-Infinite Context](https://openreview.net/pdf?id=WsRHpHH4s0)
- `ğŸ’` ğŸ“„ [(LightSeq/DistFlashAttn) DISTFLASHATTN: Distributed Memory-efficient Attention for Long-context LLMs Training](https://openreview.net/pdf?id=pUEDkZyPDl)

</details>

<details open>
<summary>

##### é€šä¿¡å‹ç¼©

</summary>

- `ğŸ’` ğŸ“„ [QSGD: Communication-Efficient SGD via Gradient Quantization and Encoding](https://proceedings.neurips.cc/paper_files/paper/2017/file/6c340f25839e6acdc73414517203f5f0-Paper.pdf)
- `ğŸ’` ğŸ“„ [Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training](https://openreview.net/pdf?id=SkhQHMW0W)
- `ğŸ’` ğŸ“„ [Don't Waste Your Bits! Squeeze Activations and Gradients for Deep Neural Networks via TinyScript](https://proceedings.mlr.press/v119/fu20c/fu20c.pdf)
- `ğŸ’` ğŸ“„ [PowerSGD: Practical Low-Rank Gradient Compression for Distributed Optimization](https://proceedings.neurips.cc/paper_files/paper/2019/file/d9fbed9da256e344c1fa46bb46c34c5f-Paper.pdf)

</details>

<details open>
<summary>

##### æ˜¾å­˜èŠ‚çº¦

</summary>

- `ğŸ’` ğŸ“„ [(Activation Checkpointing/Recomputation) Training Deep Nets with Sublinear Memory Cost](https://arxiv.org/abs/1604.06174)
- `ğŸ’` ğŸ“„ [(Offloading) vDNN: Virtualized Deep Neural Networks for Scalable, Memory-Efficient Neural Network Design](https://dl.acm.org/doi/10.5555/3195638.3195660)

</details>

<details open>
<summary>

##### ç®—å­èåˆä¼˜åŒ–

</summary>

- `ğŸ’` ğŸ“„ [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://openreview.net/pdf?id=H4DqfPSibmx)
- `ğŸ’` ğŸ“„ [FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning](https://openreview.net/pdf?id=mZn2Xyh9Ec)

</details>

<details open>
<summary>

##### é•¿åºåˆ—è®­ç»ƒ

</summary>

- `ğŸ’¡` ğŸ“„ [Striped Attention: Faster Ring Attention for Causal Transformers](https://arxiv.org/abs/2311.09431)
- `ğŸ’¡` ğŸ“„ [USP: A Unified Sequence Parallelism Approach for Long Context Generative AI](https://arxiv.org/abs/2405.07719)
- `ğŸ’¡` ğŸ“„ [LoongTrain: Efficient Training of Long-Sequence LLMs with Head-Context Parallelism](https://arxiv.org/abs/2406.18485)
- `ğŸ’¡` ğŸ“„ [Efficiently Training 7B LLM with 1 Million Sequence Length on 8 GPUs](https://arxiv.org/abs/2407.12117)


</details>

<details open>
<summary>

##### å¤šæ¨¡æ€è®­ç»ƒ

</summary>

- `ğŸ’¡` ğŸ“„ [DistTrain: Addressing Model and Data Heterogeneity with Disaggregated Training for Multimodal Large Language Models](https://arxiv.org/abs/2408.04275)
- `ğŸ’¡` ğŸ“„ [Optimus: Accelerating Large-Scale Multi-Modal LLM Training by Bubble Exploitation](https://arxiv.org/abs/2408.03505)
- `ğŸ’¡` ğŸ“„ [Efficient Multi-Task Large Model Training via Data Heterogeneity-aware Model Management](https://arxiv.org/abs/2409.03365)

</details>

<details open>
<summary>

##### å¼‚æ„è®­ç»ƒ

</summary>

- `ğŸ’¡` ğŸ“„ [HetHub: A Heterogeneous distributed hybrid training system for large-scale models](https://arxiv.org/abs/2405.16256)
- `ğŸ’¡` ğŸ“„ [AMP: Automatically Finding Model Parallel Strategies with Heterogeneity Awareness](https://proceedings.neurips.cc/paper_files/paper/2022/file/2b4bfa1cebe78d125fefd7ea6ffcfc6d-Paper-Conference.pdf)
- `ğŸ’¡` ğŸ“„ [AccPar: Tensor Partitioning for Heterogeneous Deep Learning Accelerators](https://ieeexplore.ieee.org/document/9065574)
- `ğŸ’¡` ğŸ“„ [Whale: Scaling Deep Learning Model Training to the Trillions](https://www.usenix.org/system/files/atc22-jia-xianyan.pdf)

</details>

### LLMæ¨ç†æœåŠ¡

<details open>
<summary>

##### LLMæœåŠ¡ç³»ç»Ÿæ¡†æ¶

</summary>

- `âš¡` ğŸ› ï¸ [vLLM](https://github.com/vllm-project/vllm)

</details>

<details open>
<summary>

##### ç»¼è¿°

</summary>

- `âš¡` ğŸ“„ [A Survey on Efficient Inference for Large Language Models](https://arxiv.org/abs/2404.14294)
- `âš¡` ğŸ“„ [Towards Efficient Generative Large Language Model Serving: A Survey from Algorithms to Systems](https://arxiv.org/abs/2312.15234)

</details>

<details open>
<summary>

##### Batching

</summary>

- `âš¡` ğŸ“„ [Orca: A Distributed Serving System for Transformer-Based Generative Models](https://www.usenix.org/system/files/osdi22-yu.pdf)

</details>

<details open>
<summary>

##### æ¨¡å‹å¹¶è¡Œ

</summary>

> åˆ†å¸ƒå¼æ¨ç†ä¸­ä¹Ÿæœ‰ç”¨åˆ°å¾ˆå¤šå¹¶è¡Œç­–ç•¥ï¼Œå»ºè®®å¯¹å‰é¢å…³äºå¹¶è¡Œç­–ç•¥çš„å†…å®¹ä¹Ÿè¿›è¡Œé˜…è¯»äº†è§£

- `âš¡` ğŸ“„ [AlpaServe: Statistical Multiplexing with Model Parallelism for Deep Learning Serving](https://www.usenix.org/system/files/osdi23-li-zhuohan.pdf)

</details>

<details open>
<summary>

##### æ˜¾å­˜ç®¡ç†

</summary>

- `âš¡` ğŸ“„ [Efficient Memory Management for Large Language Model Serving with PagedAttention](https://dl.acm.org/doi/10.1145/3600006.3613165)

</details>

<details open>
<summary>

##### æŠ•æœºæ¨ç†

</summary>

- `âš¡` ğŸ“„ [Fast Inference from Transformers via Speculative Decoding](https://openreview.net/pdf?id=C9NEblP8vS)

</details>

<details open>
<summary>

##### Prefix Sharing

</summary>

- `âš¡` ğŸ“„ [SGLang: Efficient Execution of Structured Language Model Programs](https://arxiv.org/abs/2312.07104)

</details>

<details open>
<summary>

##### PDåˆ†ç¦»

</summary>

- `ğŸ’` ğŸ“„ [DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving](https://www.usenix.org/system/files/osdi24-zhong-yinmin.pdf)
- `ğŸ’` ğŸ“„ [Splitwise: Efficient generative LLM inference using phase splitting](https://www.computer.org/csdl/proceedings-article/isca/2024/265800a118/1Z3pCOHPSuc)
- `ğŸ’` ğŸ“„ [Inference without Interference: Disaggregate LLM Inference for Mixed Downstream Workloads](https://arxiv.org/abs/2401.11181)
- `ğŸ’` ğŸ“„ [Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving](https://arxiv.org/abs/2407.00079)

</details>

<details open>
<summary>

##### Chunked Prefill

</summary>

- `ğŸ’` ğŸ“„ [Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve](https://www.usenix.org/system/files/osdi24-agrawal.pdf)

</details>

<details open>
<summary>

##### æ¨¡å‹å‹ç¼©

</summary>

- `ğŸ’` ğŸ“„ [GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers](https://openreview.net/pdf?id=tcbBPnfwxS)
- `ğŸ’` ğŸ“„ [AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration](https://proceedings.mlsys.org/paper_files/paper/2024/file/42a452cbafa9dd64e9ba4aa95cc1ef21-Paper-Conference.pdf)
- `ğŸ’` ğŸ“„ [SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models](https://proceedings.mlr.press/v202/xiao23c/xiao23c.pdf)


</details>

### Diffusionï¼ˆæ–‡ç”Ÿå›¾ã€æ–‡ç”Ÿè§†é¢‘ï¼‰æ¨ç†æœåŠ¡

<details open>
<summary>

##### DiTç³»ç»Ÿæ¡†æ¶

</summary>

- `âš¡` ğŸ› ï¸ [xDiT](https://github.com/xdit-project/xDiT)
- `âš¡` ğŸ› ï¸ [VideoSys](https://github.com/NUS-HPC-AI-Lab/VideoSys)

</details>

<details open>
<summary>

##### åˆ†å¸ƒå¼æ¨ç†

</summary>

- `âš¡` ğŸ“„ [DistriFusion: Distributed Parallel Inference for High-Resolution Diffusion Models](https://openaccess.thecvf.com/content/CVPR2024/papers/Li_DistriFusion_Distributed_Parallel_Inference_for_High-Resolution_Diffusion_Models_CVPR_2024_paper.pdf)
- `âš¡` ğŸ“„ [PipeFusion: Displaced Patch Pipeline Parallelism for Inference of Diffusion Transformer Models](https://arxiv.org/abs/2405.14430)
</details>

